---
- &qwen25coder
  name: "Qwen2.5-Coder-1.5B"
  icon: https://avatars.githubusercontent.com/u/141221163
  url: "github:mudler/LocalAI/gallery/chatml.yaml@master"
  license: apache-2.0
  tags:
    - llm
    - gguf
    - gpu
    - qwen
    - qwen2.5
    - cpu
  urls:
    - https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B
    - https://huggingface.co/alphaduriendur/Qwen2.5-Coder-1.5B-Q8_0-GGUF
  description: |
    Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:

        Significantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.
        A more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.
        Long-context Support up to 128K tokens.
  overrides:
    parameters:
      model: qwen2.5-coder-1.5b-q8_0.gguf
  files:
    - filename: qwen2.5-coder-1.5b-q8_0.gguf
      sha256: 373bf77657c9df66f1858c75788fa304cd5bbbbf336c1da19c4a0a84ceb10b42
      uri: "huggingface://alphaduriendur/Qwen2.5-Coder-1.5B-Q8_0-GGUF/qwen2.5-coder-1.5b-q8_0.gguf"
- !!merge <<: *qwen25coder
  name: "Qwen2.5-Coder-1.5B-Instruct"
  icon: https://avatars.githubusercontent.com/u/141221163
  url: "github:mudler/LocalAI/gallery/chatml.yaml@master"
  license: apache-2.0
  tags:
    - llm
    - gguf
    - gpu
    - qwen
    - qwen2.5
    - cpu
  urls:
    - https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct
    - https://huggingface.co/alphaduriendur/Qwen2.5-Coder-1.5B-Instruct-Q8_0-GGUF
  description: |
    Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:

        Significantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.
        A more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.
        Long-context Support up to 128K tokens.
  overrides:
    parameters:
      model: qwen2.5-coder-1.5b-instruct-q8_0.gguf
  files:
    - filename: qwen2.5-coder-1.5b-instruct-q8_0.gguf
      sha256: f68863a5458c74a0d09ba3fef7c67fffcdc8b08d0acc65b06bcf2c9b61826979
      uri: "huggingface://alphaduriendur/Qwen2.5-Coder-1.5B-Instruct-Q8_0-GGUF/qwen2.5-coder-1.5b-instruct-q8_0.gguf"
- !!merge <<: *qwen25coder
  name: "Qwen2.5-Coder-3B-Instruct"
  icon: https://avatars.githubusercontent.com/u/141221163
  url: "github:mudler/LocalAI/gallery/chatml.yaml@master"
  license: apache-2.0
  tags:
    - llm
    - gguf
    - gpu
    - qwen
    - qwen2.5
    - cpu
  urls:
    - https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct
    - https://huggingface.co/alphaduriendur/Qwen2.5-Coder-3B-Instruct-Q4_0-GGUF
  description: |
    Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:

        Significantly improvements in code generation, code reasoning and code fixing. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.
        A more comprehensive foundation for real-world applications such as Code Agents. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.
        Long-context Support up to 128K tokens.
  overrides:
    parameters:
      model: qwen2.5-coder-3b-instruct-q4_0.gguf
  files:
    - filename: qwen2.5-coder-3b-instruct-q4_0.gguf
      sha256: ac675f3274c209b0d38a00b28614589d3c29a4ecc1480463bd0ceba40aa5deff
      uri: "huggingface://alphaduriendur/Qwen2.5-Coder-3B-Instruct-Q4_0-GGUF/qwen2.5-coder-3b-instruct-q4_0.gguf"
- &gemma
  url: "github:mudler/LocalAI/gallery/gemma.yaml@master"
  name: "gemma-2b-chat"
  urls:
    - https://ai.google.dev/gemma/docs  
    - https://huggingface.co/alphaduriendur/gemma-2b-it-Q4_K_M-GGUF
  icon: https://avatars.githubusercontent.com/u/1342004
  license: gemma
  tags:
    - llm
    - gguf
    - gpu
    - cpu
    - gemma
  description: |
    Open source LLM from Google (Instruction Tuned)
  overrides:
    parameters:
      model: gemma-2b-it-q4_k_m.gguf
  files:
    - filename: gemma-2b-it-q4_k_m.gguf
      sha256: 21d5e00ca1795b5cfd535b1b38d536ff142845c80fde0638110186a3899a9c17
      uri: huggingface://alphaduriendur/gemma-2b-it-Q4_K_M-GGUF/gemma-2b-it-q4_k_m.gguf
- &llama3
  url: "github:mudler/LocalAI/gallery/llama3.2-quantized.yaml@master"
  icon: https://avatars.githubusercontent.com/u/153379578
  name: "Llama-3.2-3B-Instruct"
  license: llama3
  description: |
    The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). 
    
    The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.
  urls:
    - https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct
    - https://huggingface.co/alphaduriendur/Llama-3.2-3B-Instruct-Q4_K_M-GGUF
  tags:
    - llm
    - gguf
    - gpu
    - cpu
    - llama3
  overrides:
    parameters:
      model: llama-3.2-3b-instruct-q4_k_m.gguf
  files:
    - filename: llama-3.2-3b-instruct-q4_k_m.gguf
      uri: huggingface://alphaduriendur/Llama-3.2-3B-Instruct-Q4_K_M-GGUF/llama-3.2-3b-instruct-q4_k_m.gguf
      sha256: c5e6cf2c071a429e2380d2d1182c06a20b8c999d0c059636530e4a84beb8e8d4
- !!merge <<: *llama3
  url: "github:mudler/LocalAI/gallery/llama3.2-quantized.yaml@master"
  name: "Llama-3.2-3B"
  urls:
    - https://huggingface.co/meta-llama/Llama-3.2-3B
    - https://huggingface.co/alphaduriendur/Llama-3.2-3B-Q4_K_M-GGUF
  icon: https://avatars.githubusercontent.com/u/153379578
  license: llama3
  tags:
    - llm
    - gguf
    - gpu
    - cpu
    - llama3
  description: |
    The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). 
    The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. 
    They outperform many of the available open source and closed chat models on common industry benchmarks.
  overrides:
    parameters:
      model: llama-3.2-3b-q4_k_m.gguf
  files:
    - filename: llama-3.2-3b-q4_k_m.gguf
      sha256: 3d5329855afbf123308eb1ff2e322db6ef1066f17aaa35cd5661838cb92a12e6
      uri: huggingface://alphaduriendur/Llama-3.2-3B-Q4_K_M-GGUF/llama-3.2-3b-q4_k_m.gguf
- !!merge <<: *llama3
  url: "github:mudler/LocalAI/gallery/llama3.2-quantized.yaml@master"
  name: "Llama-3.2-1B"
  urls:
    - https://huggingface.co/meta-llama/Llama-3.2-1B
    - https://huggingface.co/alphaduriendur/Llama-3.2-1B-Q4_K_M-GGUF
  icon: https://avatars.githubusercontent.com/u/153379578
  license: llama3
  tags:
    - llm
    - gguf
    - gpu
    - cpu
    - llama3
  description: |
    The Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). 
    The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. 
    They outperform many of the available open source and closed chat models on common industry benchmarks.
  overrides:
    parameters:
      model: llama-3.2-1b-q4_k_m.gguf
  files:
    - filename: llama-3.2-1b-q4_k_m.gguf
      sha256: f8c965497de4f18749a9e07a58a28c1fd60d00866da392f10a5e49ec3d0318ab
      uri: huggingface://alphaduriendur/Llama-3.2-1B-Q4_K_M-GGUF/llama-3.2-1b-q4_k_m.gguf
- &deepseek
  url: "github:mudler/LocalAI/gallery/deepseek.yaml@master" ## Deepseek
  name: "deepseek-coder-1.3b-instruct"
  icon: "https://avatars.githubusercontent.com/u/148330874"
  license: deepseek
  description: |
    DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from DeepSeek-Coder-V2-Base with 6 trillion tokens sourced from a high-quality and multi-source corpus. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-Coder-V2-Base, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K.
    In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks. The list of supported programming languages can be found in the paper.
  urls:
    - https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-instruct
    - https://huggingface.co/alphaduriendur/deepseek-coder-1.3b-instruct-Q4_K_M-GGUF
  tags:
    - llm
    - gguf
    - gpu
    - deepseek
    - cpu
  overrides:
    parameters:
      model: deepseek-coder-1.3b-instruct-q4_k_m.gguf
  files:
    - filename: deepseek-coder-1.3b-instruct-q4_k_m.gguf
      sha256: d8c7e0d0cb722df34bc906bac28e2fc4065dc76638c67e08271710754b5102ca
      uri: huggingface://alphaduriendur/deepseek-coder-1.3b-instruct-Q4_K_M-GGUF/deepseek-coder-1.3b-instruct-q4_k_m.gguf
- !!merge <<: *deepseek
  url: "github:mudler/LocalAI/gallery/deepseek.yaml@master" ## Deepseek
  name: "deepseek-coder-6.7b-instruct"
  icon: "https://avatars.githubusercontent.com/u/148330874"
  license: deepseek
  description: |
    DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from DeepSeek-Coder-V2-Base with 6 trillion tokens sourced from a high-quality and multi-source corpus. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-Coder-V2-Base, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K.
    In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks. The list of supported programming languages can be found in the paper.
  urls:
    - https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct
    - https://huggingface.co/alphaduriendur/deepseek-coder-6.7b-instruct-Q4_K_M-GGUF
  tags:
    - llm
    - gguf
    - gpu
    - deepseek
    - cpu
  overrides:
    parameters:
      model: deepseek-coder-6.7b-instruct-q4_k_m.gguf
  files:
    - filename: deepseek-coder-6.7b-instruct-q4_k_m.gguf
      sha256: 864505d0bde7349eb11fcad62a834085bb47f34227e076cd6d8f55e8cd1b8ce4
      uri: huggingface://alphaduriendur/deepseek-coder-6.7b-instruct-Q4_K_M-GGUF/deepseek-coder-6.7b-instruct-q4_k_m.gguf